{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kj5NWKqQhEqB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BVtx8hTLhH3x"
      },
      "outputs": [],
      "source": [
        "CONTEXT_SIZE = 4\n",
        "EMBEDDING_DIM = 200\n",
        "# We will use Shakespeare Sonnet 2\n",
        "train_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "\n",
        "fivegram = [((train_sentence[i], train_sentence[i + 1], train_sentence[i + 2], train_sentence[i + 3]), train_sentence[i + 4])\n",
        "           for i in range(len(train_sentence) - 4)]\n",
        "\n",
        "vocb = set(train_sentence)\n",
        "vocb.add('<UNK>')\n",
        "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
        "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "R-JQTZ5zhNx4"
      },
      "outputs": [],
      "source": [
        "class NgramModel(nn.Module):\n",
        "    def __init__(self, vocb_size, context_size, n_dim):\n",
        "        super(NgramModel, self).__init__()\n",
        "        self.n_word = vocb_size\n",
        "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
        "        self.linear1 = nn.Linear(context_size * n_dim, 300)\n",
        "        self.linear2 = nn.Linear(300, 300)\n",
        "        self.linear3 = nn.Linear(300, self.n_word)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(1, -1)\n",
        "        out = self.linear1(emb)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear3(out)\n",
        "        log_prob = F.softmax(out, dim = 1)\n",
        "        return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "VZialrCGhSkB"
      },
      "outputs": [],
      "source": [
        "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJMH9GUrhXDX",
        "outputId": "b84dea7d-ca3a-4faa-ee9c-1221dd2f14d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1\n",
            "**********\n",
            "Loss: -0.011666\n",
            "epoch: 2\n",
            "**********\n",
            "Loss: -0.011669\n",
            "epoch: 3\n",
            "**********\n",
            "Loss: -0.011672\n",
            "epoch: 4\n",
            "**********\n",
            "Loss: -0.011675\n",
            "epoch: 5\n",
            "**********\n",
            "Loss: -0.011678\n",
            "epoch: 6\n",
            "**********\n",
            "Loss: -0.011681\n",
            "epoch: 7\n",
            "**********\n",
            "Loss: -0.011684\n",
            "epoch: 8\n",
            "**********\n",
            "Loss: -0.011687\n",
            "epoch: 9\n",
            "**********\n",
            "Loss: -0.011690\n",
            "epoch: 10\n",
            "**********\n",
            "Loss: -0.011693\n",
            "epoch: 11\n",
            "**********\n",
            "Loss: -0.011695\n",
            "epoch: 12\n",
            "**********\n",
            "Loss: -0.011698\n",
            "epoch: 13\n",
            "**********\n",
            "Loss: -0.011701\n",
            "epoch: 14\n",
            "**********\n",
            "Loss: -0.011704\n",
            "epoch: 15\n",
            "**********\n",
            "Loss: -0.011707\n",
            "epoch: 16\n",
            "**********\n",
            "Loss: -0.011710\n",
            "epoch: 17\n",
            "**********\n",
            "Loss: -0.011713\n",
            "epoch: 18\n",
            "**********\n",
            "Loss: -0.011716\n",
            "epoch: 19\n",
            "**********\n",
            "Loss: -0.011719\n",
            "epoch: 20\n",
            "**********\n",
            "Loss: -0.011722\n",
            "epoch: 21\n",
            "**********\n",
            "Loss: -0.011725\n",
            "epoch: 22\n",
            "**********\n",
            "Loss: -0.011728\n",
            "epoch: 23\n",
            "**********\n",
            "Loss: -0.011731\n",
            "epoch: 24\n",
            "**********\n",
            "Loss: -0.011733\n",
            "epoch: 25\n",
            "**********\n",
            "Loss: -0.011736\n",
            "epoch: 26\n",
            "**********\n",
            "Loss: -0.011739\n",
            "epoch: 27\n",
            "**********\n",
            "Loss: -0.011742\n",
            "epoch: 28\n",
            "**********\n",
            "Loss: -0.011745\n",
            "epoch: 29\n",
            "**********\n",
            "Loss: -0.011748\n",
            "epoch: 30\n",
            "**********\n",
            "Loss: -0.011751\n",
            "epoch: 31\n",
            "**********\n",
            "Loss: -0.011754\n",
            "epoch: 32\n",
            "**********\n",
            "Loss: -0.011757\n",
            "epoch: 33\n",
            "**********\n",
            "Loss: -0.011760\n",
            "epoch: 34\n",
            "**********\n",
            "Loss: -0.011763\n",
            "epoch: 35\n",
            "**********\n",
            "Loss: -0.011766\n",
            "epoch: 36\n",
            "**********\n",
            "Loss: -0.011769\n",
            "epoch: 37\n",
            "**********\n",
            "Loss: -0.011772\n",
            "epoch: 38\n",
            "**********\n",
            "Loss: -0.011775\n",
            "epoch: 39\n",
            "**********\n",
            "Loss: -0.011778\n",
            "epoch: 40\n",
            "**********\n",
            "Loss: -0.011780\n",
            "epoch: 41\n",
            "**********\n",
            "Loss: -0.011783\n",
            "epoch: 42\n",
            "**********\n",
            "Loss: -0.011786\n",
            "epoch: 43\n",
            "**********\n",
            "Loss: -0.011789\n",
            "epoch: 44\n",
            "**********\n",
            "Loss: -0.011792\n",
            "epoch: 45\n",
            "**********\n",
            "Loss: -0.011795\n",
            "epoch: 46\n",
            "**********\n",
            "Loss: -0.011798\n",
            "epoch: 47\n",
            "**********\n",
            "Loss: -0.011801\n",
            "epoch: 48\n",
            "**********\n",
            "Loss: -0.011804\n",
            "epoch: 49\n",
            "**********\n",
            "Loss: -0.011807\n",
            "epoch: 50\n",
            "**********\n",
            "Loss: -0.011810\n",
            "epoch: 51\n",
            "**********\n",
            "Loss: -0.011813\n",
            "epoch: 52\n",
            "**********\n",
            "Loss: -0.011816\n",
            "epoch: 53\n",
            "**********\n",
            "Loss: -0.011819\n",
            "epoch: 54\n",
            "**********\n",
            "Loss: -0.011822\n",
            "epoch: 55\n",
            "**********\n",
            "Loss: -0.011825\n",
            "epoch: 56\n",
            "**********\n",
            "Loss: -0.011828\n",
            "epoch: 57\n",
            "**********\n",
            "Loss: -0.011831\n",
            "epoch: 58\n",
            "**********\n",
            "Loss: -0.011834\n",
            "epoch: 59\n",
            "**********\n",
            "Loss: -0.011837\n",
            "epoch: 60\n",
            "**********\n",
            "Loss: -0.011840\n",
            "epoch: 61\n",
            "**********\n",
            "Loss: -0.011843\n",
            "epoch: 62\n",
            "**********\n",
            "Loss: -0.011846\n",
            "epoch: 63\n",
            "**********\n",
            "Loss: -0.011849\n",
            "epoch: 64\n",
            "**********\n",
            "Loss: -0.011852\n",
            "epoch: 65\n",
            "**********\n",
            "Loss: -0.011855\n",
            "epoch: 66\n",
            "**********\n",
            "Loss: -0.011858\n",
            "epoch: 67\n",
            "**********\n",
            "Loss: -0.011861\n",
            "epoch: 68\n",
            "**********\n",
            "Loss: -0.011864\n",
            "epoch: 69\n",
            "**********\n",
            "Loss: -0.011867\n",
            "epoch: 70\n",
            "**********\n",
            "Loss: -0.011870\n",
            "epoch: 71\n",
            "**********\n",
            "Loss: -0.011873\n",
            "epoch: 72\n",
            "**********\n",
            "Loss: -0.011876\n",
            "epoch: 73\n",
            "**********\n",
            "Loss: -0.011879\n",
            "epoch: 74\n",
            "**********\n",
            "Loss: -0.011882\n",
            "epoch: 75\n",
            "**********\n",
            "Loss: -0.011885\n",
            "epoch: 76\n",
            "**********\n",
            "Loss: -0.011888\n",
            "epoch: 77\n",
            "**********\n",
            "Loss: -0.011891\n",
            "epoch: 78\n",
            "**********\n",
            "Loss: -0.011894\n",
            "epoch: 79\n",
            "**********\n",
            "Loss: -0.011897\n",
            "epoch: 80\n",
            "**********\n",
            "Loss: -0.011900\n",
            "epoch: 81\n",
            "**********\n",
            "Loss: -0.011903\n",
            "epoch: 82\n",
            "**********\n",
            "Loss: -0.011906\n",
            "epoch: 83\n",
            "**********\n",
            "Loss: -0.011909\n",
            "epoch: 84\n",
            "**********\n",
            "Loss: -0.011912\n",
            "epoch: 85\n",
            "**********\n",
            "Loss: -0.011915\n",
            "epoch: 86\n",
            "**********\n",
            "Loss: -0.011918\n",
            "epoch: 87\n",
            "**********\n",
            "Loss: -0.011921\n",
            "epoch: 88\n",
            "**********\n",
            "Loss: -0.011924\n",
            "epoch: 89\n",
            "**********\n",
            "Loss: -0.011927\n",
            "epoch: 90\n",
            "**********\n",
            "Loss: -0.011930\n",
            "epoch: 91\n",
            "**********\n",
            "Loss: -0.011933\n",
            "epoch: 92\n",
            "**********\n",
            "Loss: -0.011936\n",
            "epoch: 93\n",
            "**********\n",
            "Loss: -0.011939\n",
            "epoch: 94\n",
            "**********\n",
            "Loss: -0.011942\n",
            "epoch: 95\n",
            "**********\n",
            "Loss: -0.011945\n",
            "epoch: 96\n",
            "**********\n",
            "Loss: -0.011948\n",
            "epoch: 97\n",
            "**********\n",
            "Loss: -0.011951\n",
            "epoch: 98\n",
            "**********\n",
            "Loss: -0.011954\n",
            "epoch: 99\n",
            "**********\n",
            "Loss: -0.011957\n",
            "epoch: 100\n",
            "**********\n",
            "Loss: -0.011960\n",
            "tensor([[0.0126, 0.0112, 0.0090, 0.0112, 0.0101, 0.0089, 0.0111, 0.0115, 0.0113,\n",
            "         0.0104, 0.0086, 0.0106, 0.0092, 0.0103, 0.0120, 0.0100, 0.0091, 0.0119,\n",
            "         0.0098, 0.0102, 0.0098, 0.0105, 0.0102, 0.0107, 0.0098, 0.0106, 0.0091,\n",
            "         0.0086, 0.0106, 0.0098, 0.0105, 0.0113, 0.0091, 0.0124, 0.0092, 0.0109,\n",
            "         0.0097, 0.0098, 0.0114, 0.0087, 0.0119, 0.0112, 0.0092, 0.0094, 0.0094,\n",
            "         0.0098, 0.0102, 0.0086, 0.0099, 0.0098, 0.0099, 0.0118, 0.0100, 0.0102,\n",
            "         0.0100, 0.0100, 0.0111, 0.0116, 0.0093, 0.0106, 0.0101, 0.0105, 0.0089,\n",
            "         0.0093, 0.0103, 0.0095, 0.0131, 0.0116, 0.0082, 0.0095, 0.0096, 0.0086,\n",
            "         0.0115, 0.0099, 0.0106, 0.0097, 0.0092, 0.0099, 0.0094, 0.0096, 0.0095,\n",
            "         0.0104, 0.0109, 0.0112, 0.0109, 0.0100, 0.0089, 0.0107, 0.0102, 0.0091,\n",
            "         0.0110, 0.0084, 0.0104, 0.0099, 0.0107, 0.0108, 0.0120, 0.0104]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0.0131], grad_fn=<MaxBackward0>)\n",
            "66\n",
            "real word is dig, predict word is within\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(100):\n",
        "    print('epoch: {}'.format(epoch + 1))\n",
        "    print('*' * 10)\n",
        "    running_loss = 0\n",
        "    for data in fivegram:\n",
        "        word, label = data\n",
        "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
        "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
        "        # forward\n",
        "        out = ngrammodel(word)\n",
        "        loss = criterion(out, label)\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))\n",
        "\n",
        "word, label = fivegram[4]\n",
        "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
        "out = ngrammodel(word)\n",
        "print(out)\n",
        "prob, predict_label = torch.max(out, 1)\n",
        "print(prob)\n",
        "print(predict_label.item())\n",
        "predict_word = idx_to_word[predict_label.item()]\n",
        "print('real word is {}, predict word is {}'.format(label, predict_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTD5zHvolj8B",
        "outputId": "174a0aac-7340-4e8e-abe2-2178c2156ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "besiege\n",
            "thy\n",
            "brow,\n",
            "And\n"
          ]
        }
      ],
      "source": [
        "for i in word:\n",
        "  print(idx_to_word[i.item()])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
